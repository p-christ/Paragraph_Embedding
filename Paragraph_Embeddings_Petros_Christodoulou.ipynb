{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paragraph Embeddings - Petros Christodoulou.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWdnkNrkdTFf",
        "colab_type": "text"
      },
      "source": [
        "# **Paragraph Embeddings - Petros Christodoulou**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrYvcGYiFtUX",
        "colab_type": "text"
      },
      "source": [
        "To create a paragraph embedding we will take a different approach that takes advantage of more recent developments in NLP while dealing with the weaknesses of PV-DBOW and PV-DM. The steps in our approach will be:\n",
        "\n",
        "1.   Split a given paragraph up into its **sentences**\n",
        "2.   **Embed each sentence** using a pre-trained **BERT** model (or some other recent embedding model). This is a form of transfer learning and doing this lets us take advantage of the masses amount of data and compute time used to train BERT.\n",
        "3.   **Feed each embedded sentence into the encoder GRU** (or LSTM) that will be used to **produce the paragraph embedding e**. We use a GRU or LSTM because: \n",
        "a) The sentences are in sequential order and so we wish to take advantage of this information and b) Using a GRU/LSTM lets our approach be independent of the length of the paragraph / document.\n",
        "4.   Take the **output of the encoder GRU (e)** and **feed it into the decoder GRU** that will try to recreate the sequence of BERT embeddings\n",
        "5.   Train the combined model using the **mean squared error loss on the difference between the input to the encoder GRU and the output of the decoder GRU**. Doing this means we are training the model to produce a paragraph embedding e that maintains as much information as possible about the paragraph so that the decoder GRU is able to recreate the sentences in the input paragraph.\n",
        "6.   After the model is trained it **can be used to generate paragraph embeddings given by the output of the encoder GRU (e)**. This will work for any size paragraph or document. There also will not be any training required at test time, therefore dealing with the biggest problem with the PV-DBOW and PV-DM methods\n",
        "\n",
        "\n",
        "Below we provide a diagram demonstrating the model in more detail as well as a PyTorch implementation:\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1SSuGAnClC6OvblnTIB5-q292ncnvMp41)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur4qhxDYgRBO",
        "colab_type": "text"
      },
      "source": [
        "## **PyTorch Implementation** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG7skuPEdRhd",
        "colab_type": "code",
        "outputId": "cba4d72a-cc76-4332-f409-321052e5ae28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# We download this package so we can use a pre-trained BERT model\n",
        "!pip install transformers -q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 317kB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 57.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 40.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 860kB 42.4MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvEJUNIJMiyI",
        "colab_type": "code",
        "outputId": "4e2f97bb-74f2-4b78-b9a5-e41d3a429b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "\n",
        "paragraph = \"My first sentence. My second sentence. My third sentence\"\n",
        "sentences = paragraph.split(\".\")\n",
        "\n",
        "# Load the BERT tokenizer which prepares a sentence for usage in a BERT model\n",
        "tokenizer = tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Now we encode each sentence so it is ready to be embedded by the BERT model \n",
        "tokenized_sentences = []\n",
        "for ix, sentence in enumerate(sentences):\n",
        "  tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "  print(\"Tokenized sentence {}: {}\".format(ix, tokenized_sentence))\n",
        "  tokenized_sentences.append(tokenized_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized sentence 0: [101, 1422, 1148, 5650, 102]\n",
            "Tokenized sentence 1: [101, 1422, 1248, 5650, 102]\n",
            "Tokenized sentence 2: [101, 1422, 1503, 5650, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x51jxonsPNbU",
        "colab_type": "code",
        "outputId": "197c492c-9e17-4f48-d428-211722cbd19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "hidden_size_encode = 768\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    nn.Module.__init__(self)\n",
        "\n",
        "    # We load the pretrained BERT model\n",
        "    self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "    \n",
        "    # This creates the GRU layer that will embed the paragraph. Its input size is 768 \n",
        "    # because that is the size of embedding outputted by BERT for a sentence\n",
        "    self.gru_encode = nn.GRU(input_size=768, hidden_size=hidden_size_encode, batch_first=True)\n",
        "\n",
        "    # This creates the GRU layer that will try and recover the sentence BERT embeddings\n",
        "    # from the paragraph embedding\n",
        "    self.gru_decode = nn.GRU(input_size=hidden_size_encode, hidden_size=768, batch_first=True)\n",
        "\n",
        "  def forward(self, x, verbose=False):\n",
        "    # First we embed all the sentences using BERT\n",
        "    if verbose: print(\"Input data shape \", x.shape)    \n",
        "    encoded_sentences = self.bert(x)[1] \n",
        "    if verbose: print(\"Encoded sentences shape \", encoded_sentences.shape)\n",
        "\n",
        "    # Then we put the sentence embeddings through a GRU to form a paragraph embedding\n",
        "    encoded_paragraph = self.gru_encode(encoded_sentences.unsqueeze(0))[-1][0, :, :]\n",
        "    if verbose: print(\"Paragraph embedding shape \", encoded_paragraph.shape)\n",
        "\n",
        "    # Then we try and recover the BERT sentence embedddings using the paragraph embedding\n",
        "    # and another GRU\n",
        "    decoded_sentences = []\n",
        "    inputs = encoded_paragraph\n",
        "\n",
        "    # This loop occurs because we feed in the predictions of the previous timestep as the \n",
        "    # input into the next timestep\n",
        "    for _ in range(x.shape[0]):\n",
        "      output = self.gru_decode(inputs.unsqueeze(0))[-1][0, :, :]\n",
        "      decoded_sentences.append(output)\n",
        "      inputs = output  \n",
        "    decoded_sentences = torch.cat(decoded_sentences)\n",
        "    if verbose: print(\"Decoded sentences shape \", decoded_sentences.shape)\n",
        "    return encoded_sentences, encoded_paragraph, decoded_sentences\n",
        "\n",
        "model = Model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 64540.44B/s]\n",
            "100%|██████████| 435779157/435779157 [00:15<00:00, 27532138.97B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USxZaetpiAb9",
        "colab_type": "code",
        "outputId": "5519c5ef-a891-48a4-9284-c625c7a2fed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# The example below shows the shapes of the data that pass through the network\n",
        "x = torch.LongTensor(tokenized_sentences) \n",
        "encoded_sentences, encoded_paragraph, decoded_sentences = model(x, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data shape  torch.Size([3, 5])\n",
            "Encoded sentences shape  torch.Size([3, 768])\n",
            "Paragraph embedding shape  torch.Size([1, 768])\n",
            "Decoded sentences shape  torch.Size([3, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Zd3ukZeK6x",
        "colab_type": "code",
        "outputId": "e25a599b-bb37-43ec-9fda-8b7de85b9508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# To train the network we run through a training loop like below\n",
        "x = torch.LongTensor(tokenized_sentences) \n",
        "for ix in range(10):\n",
        "  encoded_sentences, encoded_paragraph, decoded_sentences = model(x)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "  loss = torch.nn.MSELoss()(encoded_sentences, decoded_sentences)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(\"Iteration {} -- Loss {}\".format(ix+1, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 -- Loss 0.6411935091018677\n",
            "Iteration 2 -- Loss 0.5397552251815796\n",
            "Iteration 3 -- Loss 0.49741649627685547\n",
            "Iteration 4 -- Loss 0.4484933316707611\n",
            "Iteration 5 -- Loss 0.3764909505844116\n",
            "Iteration 6 -- Loss 0.29390841722488403\n",
            "Iteration 7 -- Loss 0.19785046577453613\n",
            "Iteration 8 -- Loss 0.12871432304382324\n",
            "Iteration 9 -- Loss 0.10040237754583359\n",
            "Iteration 10 -- Loss 0.08216286450624466\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}